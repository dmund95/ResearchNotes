---
layout: paper
title: "DV365: Extremely Long User History Modeling at Instagram"
authors: []
year: 2025
journal: ""
paper: https://arxiv.org/pdf/2506.00450
---

## Summary
The paper focuses on scaling user sequences to super long lengths (order of 10e5) to increase click through prediction accuracy. This is done by introducing an offline upstream model that's trained daily. The user embeddings are cached in a key-value store for serving downstream model at runtime. 


## Key ideas

1. Scaling user sequence is an obvious way to improve recommendations. However naive sequence scaling hits a bottleneck in terms of logging cost and serving QPS regression. For example, at instagram online serving model uses only 500 user sequence length. 

2. Most of the competing works use retrival based strategy, which samples from a huge raw sequence length (O(10e4)) based on candidate ad and pass the retrieved user items to deeper arch to learn finer pattern. Example is TWIN paper. However such techniques still suffer from high logging costs.

3. DV365 introduces an offline upstream model that uses features logged in cold storage (which is cheaper than logging infra) and is trained daily. To handle freshness issue introduced by sporadic training, authors claim to focus only on 'stable user interests', which should be robust to low freshness.

4. Given the user interaction data of upto 50000 actions, authors manually handcraft 200 id list features. For example
    a. 'all postID with watch time > 50%'
    b. 'all authorID whose posts user liked in last 7 days'
Embedding lookup with mean pooling results in 200x256 user action tensor. To learn stable long term user embeddings, latest 24 hr interaction data is dropped.

5. Since 200 embeddings are too large to be passed downstream, these are passed through a summarization arch inspired by Funnel Transformer arch. The pooling happens across token dimension followed by attention for summarization and an LCE compression block is added in parallel. Finally 58 embeddings remain which are passed through a backbone network (similar to deep arch at runtime) with same multi-task training objective as v0 model.

<figure class="image-container">
    <img src="{{ '/assets/images/FSA.png' | relative_url }}" alt="Funnel transformer summarization block" class="paper-image">
    <figcaption class="image-caption">Funnel transformer summarization block</figcaption>
</figure>

<figure class="image-container">
    <img src="{{ '/assets/images/dv365.png' | relative_url }}" alt="DV365 model arch" class="paper-image">
    <figcaption class="image-caption">DV365 model arch</figcaption>
</figure>

4. The compressed embeddings are converted to INT4 format for storage and are de-quantized back to float embeddings during inference. 

5. After adding the user embeddings, downstream model sees 0.4% NE gains.  

## Limitations
1. Manually handcrafting 200 hand features is very tedious. An end-to-end data driver apporach will scale better in long term. 

2. Not an end-to-end model that uses super long user action sequence in real time. 